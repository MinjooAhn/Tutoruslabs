{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT API Format\n",
    "\n",
    "- Used to mass input different prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    retry_if_exception_type\n",
    ") \n",
    "# Tenacity used to prevent the api call from crashing mid loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = \"\"                ## organization id\n",
    "openai.api_key = \"\"                     ## open api key\n",
    "df = pd.read_csv(r'', header=None)      ## Read Answers from CSV file\n",
    "answers = df.iloc[1:, 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt\n",
    "Explanation_scheme_step_and_score_C1=\"\"\"\n",
    "As an essay grader, your task is to evaluate and assign a grade between 1 and 5 to the provided Korean text. Re-evaluate your thought process step by step to show how it reflects the grading scheme. Provide your grade as a single integer value.\n",
    "The text you are grading is an essay on a given essay prompt. Your evaluation should be based on the combined criteria of problem comprehension and explanation quality.\n",
    "\n",
    "Please consider the following aspects while grading:\n",
    "- Problem Comprehension: Assess how effectively the text demonstrates understanding of the problem and its relevance. Focus on the introduction and information presented throughout the text.\n",
    "- Explanation Quality: Evaluate the informativeness of the explanation provided. Ensure that the text offers sufficient information to clarify the problem concisely.\n",
    "\n",
    "Please use the following grading scheme:\n",
    "5 points: The text demonstrates a very effective understanding of the problem, is closely related to the issue, and provides necessary information.\n",
    "4 points: The text demonstrates a generally effective understanding of the problem, is closely related to the issue, and includes mostly necessary information.\n",
    "3 points: The text shows an understanding of the problem related to the issue but lacks some information.\n",
    "2 points: The text's understanding of the problem contains information that is not closely related to the issue.\n",
    "1 point: The text provides no explanation of the problem or displays a very weak understanding.\n",
    "\n",
    "Now, carefully review the following essay prompt and essay, and assign a grade accordingly:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block calling gpt4 and gpt 3.5 turbo models.\n",
    "@retry(\n",
    "    retry=retry_if_exception_type((openai.error.APIError, openai.error.APIConnectionError, openai.error.RateLimitError, openai.error.ServiceUnavailableError, openai.error.Timeout)), \n",
    "    wait=wait_random_exponential(multiplier=1, max=60), \n",
    "    stop=stop_after_attempt(10)\n",
    ")\n",
    "def get_grade_gpt4(prompt_type, QA):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt_type},\n",
    "            {\"role\": \"user\", \"content\": QA}\n",
    "        ],\n",
    "        temperature = 0.1\n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content'].strip()\n",
    "    return response_text\n",
    "\n",
    "def get_grade_gpt3(prompt_type, QA):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt_type},\n",
    "            {\"role\": \"user\", \"content\": QA}\n",
    "        ],\n",
    "        temperature = 0.1 \n",
    "        \n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content'].strip()\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT4\n",
    "# Process prompts and get responses\n",
    "C2_responses_gpt4= []\n",
    "for answer in answers:\n",
    "    response_text = get_grade_gpt4(Explanation_scheme_step_and_score_C1, answer)\n",
    "    C2_responses_gpt4.append(response_text)\n",
    "\n",
    "# Create a new DataFrame with the responses\n",
    "C2_responses_gpt4_df = pd.DataFrame({'C2_responses_gpt4_gpt3.5': C2_responses_gpt4})\n",
    "\n",
    "file_path = r''\n",
    "writer = pd.ExcelWriter(file_path, engine='openpyxl')\n",
    "\n",
    "# Logging the responses as an excel / csv file\n",
    "C2_responses_gpt4_df.to_excel(writer, index=False, header=['C2_responses_gpt4'])\n",
    "C2_responses_gpt4_df.to_csv(r\"\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
