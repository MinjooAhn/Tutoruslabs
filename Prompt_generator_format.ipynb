{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2115dae",
   "metadata": {},
   "source": [
    "## Prompt Generator\n",
    "\n",
    "- Using data stored in different files to create prompts\n",
    "- Uses printcombinedtext function to print and copy generated promtps\n",
    "- Uses prompt generator function to generate prompts into json / excel / csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ac9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to do this because I had unorganised data\n",
    "answer_df = pd.read_excel()\n",
    "question_df = pd.read_excel()\n",
    "response_df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt prefix\n",
    "\n",
    "Explanation_scheme_eng_c3 = \"\"\"\n",
    "As an essay grader, your task is to evaluate and assign a grade on a scale from 1 to 5 to the provided Korean text. Please provide your grade as a single integer value without any additional text or formatting. The text you are grading is an essay on an essay prompt, where the problem is stated within the question itself. Your evaluation should be based on the \"Coherence of Claim-Reasoning/Evidence Relationship.\"\n",
    "\n",
    "Coherence of Claim-Reasoning/Evidence Relationship: Assess whether there is a strong and logical connection established between the claim and the accompanying reasoning/evidence. Verify if the provided reasons or evidence effectively support the claim or subclaims, and if the claim aligns well with the relevant issue.\n",
    "\n",
    "Please use the following grading scheme:\n",
    "\n",
    "5 points: Exceptionally appropriate reasoning and evidence, highly persuasive.\n",
    "4 points: Reasoning and evidence are generally appropriate and persuasive.\n",
    "3 points: Reasoning and evidence are somewhat acceptable but could be improved.\n",
    "2 points: Many instances where reasoning and evidence are not appropriate or lack persuasiveness.\n",
    "1 point: Most of the reasoning and evidence are not appropriate, lacking persuasiveness.\n",
    "\n",
    "Now, carefully review the following essay prompt and essay, and assign a grade accordingly:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5fd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to check the prompt / copy into clipboard for web api use\n",
    "def print_combined_text(prompt_type, text_id):\n",
    "    text_column = answer_df[answer_df['EXAMINEE_ID'] == text_id]['TEXT'].values[0]\n",
    "    question_column = question_df[question_df['Q_NUM'] == int(text_id[1])]['Q_TEXT'].values\n",
    "    combined_text = f\"{prompt_type}\\n**Discussion point:**\\n {question_column}\\n\\n**Essay to grade:**\\n [{text_column}]\"\n",
    "    print(combined_text)\n",
    "    pyperclip.copy(combined_text)\n",
    "    print(\"Copied to clipboard!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baeadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_combined_text(exo, \"A1-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af73e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "def generate_promptqa(examinee_id, question_text, text_to_grade):\n",
    "    return pd.Series([examinee_id, f\"**질문:**\\n {question_text}\\n\\n**채점할 답변:**\\n [{text_to_grade}]\"])\n",
    "\n",
    "def generate_prompts(data):\n",
    "    prompts = []\n",
    "    for index, row in data.iterrows():\n",
    "        examinee_id = row['EXAMINEE_ID']\n",
    "        text_column = row['TEXT']\n",
    "        question_column = question_df[question_df['Q_NUM'] == int(examinee_id[1])]['Q_TEXT'].values[0]\n",
    "        prompt = generate_promptqa(examinee_id, question_column, text_column)\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return pd.concat(prompts, axis=1).T\n",
    "\n",
    "prompts_df = generate_prompts(answer_df)  # Pass the correct DataFrame object\n",
    "prompts_df.to_csv(r'', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(essay_prompt, essay, grade):\n",
    "    return {\n",
    "        \"instruction\": essay_prompt,\n",
    "        \"input\": essay,\n",
    "        \"output\": grade\n",
    "    }\n",
    "\n",
    "def generate_prompts(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    prompts = []\n",
    "    for entry in data:\n",
    "        question_number = entry['Q']\n",
    "        essay = entry['TEXT']\n",
    "        grade = entry['C1']\n",
    "\n",
    "        essay_prompt = question_df[question_df['Q_NUM'] == question_number]['Q_TEXT'].values[0]\n",
    "\n",
    "        prompt = generate_prompt(essay_prompt, essay, grade)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "\n",
    "# Set file path\n",
    "json_file_path = r\"\"\n",
    "prompts_list = generate_prompts(json_file_path)\n",
    "\n",
    "# Saving the prompts list as a JSON file\n",
    "with open(r'', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(prompts_list, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data to match LoRA templates\n",
    "\n",
    "def generate_prompt(essay_prompt, essay, grade):\n",
    "    return {\n",
    "        \"instruction\": essay_prompt,\n",
    "        \"input\": essay,\n",
    "        \"output\": grade\n",
    "    }\n",
    "\n",
    "def generate_prompts(json_file_path, excel_file_path):\n",
    "    excel_data = pd.read_excel(excel_file_path, index_col='글 자료 ID')\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    prompts = []\n",
    "    for entry in data:\n",
    "        EXAMINEE_ID = entry[\"EXAMINEE_ID\"]\n",
    "        question_number = entry['Q']\n",
    "\n",
    "        # Check if the '글 자료 ID' exists \n",
    "        if EXAMINEE_ID in excel_data.index:\n",
    "            # Get the essay prompt from the Q variable using the question_number\n",
    "            essay_prompt = question_df[question_df['Q_NUM'] == question_number]['Q_TEXT'].values[0]\n",
    "            grade = excel_data.loc[EXAMINEE_ID, 'C1_responses_gpt3.5']\n",
    "            essay = entry['TEXT']\n",
    "\n",
    "            prompt = generate_prompt(essay_prompt, essay, grade)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# Set file paths\n",
    "json_file_path = r\"\"\n",
    "excel_file_path = r\"\"\n",
    "prompts_list = generate_prompts(json_file_path, excel_file_path)\n",
    "\n",
    "# Save the prompts list as a JSON file\n",
    "with open(r'', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(prompts_list, json_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
