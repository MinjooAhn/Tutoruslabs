{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\", add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "data1 = load_dataset(\"json\", data_files=r\"\")\n",
    "data2 = load_dataset(\"json\", data_files=r\"\")\n",
    "\n",
    "# Convert the JSON strings to dictionaries using the applymap method\n",
    "def generate_prompt(essay, response):\n",
    "    try:\n",
    "        prompt = f\"\"\"수필 평가자로서 제공된 한국어 답변에 대해 1점부터 5점까지의 점수를 매기는 역할을 맡았습니다. 대답은 철저히 단계별로 사고 과정을 검토한 후, 추가 설명 없이 단일 정숫값으로 점수를 표시해 주세요.\n",
    "        평가하는 글은 주어진 질문에 대한 답변입니다. 평가는 이유와 근거의 적절성을 기반으로 해야 합니다. 점수를 매길 때 다음 측면을 고려해 주세요:\n",
    "\n",
    "        주장-이유/근거 관계의 적절성: 주장 또는 주장의 이유를 뒷받침하는 근거가 적절한지 평가해 주세요. 주장이나 주장의 이유를 충분히 지지하는 근거가 제공되었는지, 주장이 관련 문제와 일치하는지 확인해 주세요.\n",
    "\n",
    "        다음과 같은 점수 체계를 사용해 주세요:\n",
    "        5점: 이유나 근거가 매우 적절하여 설득력이 높음.\n",
    "        4점: 이유나 근거가 대체로 적절하여 다소 설득력이 있음.\n",
    "        3점: 이유나 근거 가운데 일부가 적절하지 않으나, 대체로 수용할 만함.\n",
    "        2점: 이유나 근거가 적절하지 않은 경우가 많아 설득력이 떨어짐.\n",
    "        1점: 이유나 근거가 대부분 적절하지 않아 설득력이 매우 낮음.\n",
    "\n",
    "        이제 다음 질문과 답변을 주의 깊게 검토하고 점수를 매겨 주세요:\n",
    "        ### 질문:\n",
    "        {essay[\"Essay_prompt\"]}\n",
    "        ### 답변:\n",
    "        {essay[\"Essay\"]}\n",
    "        ### 정답:\n",
    "        {response[\"Response\"]}\n",
    "        ### 점수:\n",
    "        {response[\"Grade\"]}\"\"\"\n",
    "        return prompt\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        # Handle empty or invalid response data here\n",
    "        print(f\"Error processing response data: {e}\")\n",
    "        # Provide a default prompt with empty strings for missing values in the response\n",
    "        prompt = f\"\"\"수필 평가자로서 제공된 한국어 답변에 대해 1점부터 5점까지의 점수를 매기는 역할을 맡았습니다. 대답은 철저히 단계별로 사고 과정을 검토한 후, 추가 설명 없이 단일 정숫값으로 점수를 표시해 주세요.\n",
    "        평가하는 글은 주어진 질문에 대한 답변입니다. 평가는 이유와 근거의 적절성을 기반으로 해야 합니다. 점수를 매길 때 다음 측면을 고려해 주세요:\n",
    "\n",
    "        주장-이유/근거 관계의 적절성: 주장 또는 주장의 이유를 뒷받침하는 근거가 적절한지 평가해 주세요. 주장이나 주장의 이유를 충분히 지지하는 근거가 제공되었는지, 주장이 관련 문제와 일치하는지 확인해 주세요.\n",
    "\n",
    "        다음과 같은 점수 체계를 사용해 주세요:\n",
    "        5점: 이유나 근거가 매우 적절하여 설득력이 높음.\n",
    "        4점: 이유나 근거가 대체로 적절하여 다소 설득력이 있음.\n",
    "        3점: 이유나 근거 가운데 일부가 적절하지 않으나, 대체로 수용할 만함.\n",
    "        2점: 이유나 근거가 적절하지 않은 경우가 많아 설득력이 떨어짐.\n",
    "        1점: 이유나 근거가 대부분 적절하지 않아 설득력이 매우 낮음.\n",
    "\n",
    "        이제 다음 질문과 답변을 주의 깊게 검토하고 점수를 매겨 주세요:\n",
    "        ### 질문:\n",
    "        {essay[\"Essay_prompt\"]}\n",
    "        ### 답변:\n",
    "        {essay[\"Essay\"]}\n",
    "        ### 정답:\n",
    "        [No response data available]\n",
    "        ### 점수:\n",
    "        {response[\"Grade\"]}\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "data = []\n",
    "\n",
    "# Generate and tokenize prompts from both datasets\n",
    "for (essay, response) in zip(data1['train'], data2['train']):\n",
    "    prompt = generate_prompt(essay, response)\n",
    "    tokenized_prompt = tokenizer(prompt)\n",
    "    data.append({\"prompt\": tokenized_prompt})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lengths = [len(entry[\"prompt\"][\"input_ids\"]) for entry in data]\n",
    "plt.hist(prompt_lengths, bins=100)\n",
    "plt.title(\"Distribution of prompt lengths\")\n",
    "plt.axvline(2048, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([len([l for l in prompt_lengths if l <= m]) for m in range(max(prompt_lengths) + 1)])\n",
    "plt.title(\"Number of fully covered examples as a function of max length\")\n",
    "plt.axvline(x=2048, color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of tokens left out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([sum(min(l, m) for l in prompt_lengths) for m in range(max(prompt_lengths) + 1)])\n",
    "plt.title(\"Token coverage as a function of max length\")\n",
    "plt.axvline(x=2048, color=\"red\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90bfda469df5ac7fed8d7e225d563f60a7a7aa420ccfadb091c914debf775e49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
